{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to your Chinook database\n",
    "conn = sqlite3.connect('E:\\bryan - Copy\\4.2\\dataScience\\E-COM/Chinook_Sqlite.sqlite')  # <- Update this path\n",
    "\n",
    "# Step 2: Test if the connection is working - list all tables\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query, conn)\n",
    "print(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to your Chinook database\n",
    "conn = sqlite3.connect('Chinook_Sqlite.sqlite')  # Using relative path\n",
    "\n",
    "# Step 2: Test if the connection is working - list all tables\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query, conn)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to easily preview any table\n",
    "def preview_table(table_name):\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 10;\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "# Example: Preview 'Customer' table\n",
    "preview_table('Customer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to easily preview any table\n",
    "def preview_table(table_name):\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 10;\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "# Example: Preview 'Customer' table\n",
    "preview_table('Track')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to easily preview any table\n",
    "def preview_table(table_name):\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 10;\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "# Example: Preview 'Customer' table\n",
    "preview_table('InvoiceLine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers per country\n",
    "query = \"\"\"\n",
    "SELECT Country, COUNT(CustomerId) AS CustomerCount\n",
    "FROM Customer\n",
    "GROUP BY Country\n",
    "ORDER BY CustomerCount DESC;\n",
    "\"\"\"\n",
    "customers_by_country = pd.read_sql_query(query, conn)\n",
    "customers_by_country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d742064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 most expensive tracks\n",
    "query = \"\"\"\n",
    "SELECT Name, UnitPrice\n",
    "FROM Track\n",
    "ORDER BY UnitPrice DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "top_expensive_tracks = pd.read_sql_query(query, conn)\n",
    "top_expensive_tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and total spending per customer\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.CustomerId,\n",
    "    c.FirstName || ' ' || c.LastName AS CustomerName,\n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalSpending,\n",
    "    ROUND(AVG(il.UnitPrice * il.Quantity), 2) AS AvgSpendingPerItem\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.CustomerId\n",
    "ORDER BY TotalSpending DESC;\n",
    "\"\"\"\n",
    "customer_spending = pd.read_sql_query(query, conn)\n",
    "customer_spending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue per country\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(DISTINCT c.CustomerId) AS NumCustomers\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.Country\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "revenue_by_country = pd.read_sql_query(query, conn)\n",
    "revenue_by_country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9543d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue per country\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(DISTINCT c.CustomerId) AS NumCustomers\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.Country\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "revenue_by_country = pd.read_sql_query(query, conn)\n",
    "revenue_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Revenue per country\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(DISTINCT c.CustomerId) AS NumCustomers\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.Country\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "revenue_by_country = pd.read_sql_query(query, conn)\n",
    "revenue_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to your Chinook database\n",
    "conn = sqlite3.connect('E:\\bryan - Copy\\4.2\\dataScience\\E-COM/Chinook_Sqlite.sqlite')  # <- Update this path\n",
    "# Revenue per country\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(DISTINCT c.CustomerId) AS NumCustomers\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.Country\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "revenue_by_country = pd.read_sql_query(query, conn)\n",
    "revenue_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to your Chinook database\n",
    "conn = sqlite3.connect('Chinook_Sqlite.sqlite')  # Using relative path\n",
    "\n",
    "# Revenue per country\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.Country,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(DISTINCT c.CustomerId) AS NumCustomers\n",
    "FROM Customer c\n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n",
    "GROUP BY c.Country\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "revenue_by_country = pd.read_sql_query(query, conn)\n",
    "revenue_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Sort the dataframe if not already sorted\n",
    "revenue_by_country = revenue_by_country.sort_values(by='TotalRevenue', ascending=False)\n",
    "\n",
    "# Set up plot size and style\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create barplot\n",
    "sns.barplot(data=revenue_by_country, x='Country', y='TotalRevenue', palette='viridis')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Total Revenue by Country', fontsize=16)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Revenue (USD)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query and load into a DataFrame\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    g.Name AS Genre,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(il.InvoiceLineId) AS TotalSales\n",
    "FROM \n",
    "    InvoiceLine il\n",
    "JOIN \n",
    "    Track t ON il.TrackId = t.TrackId\n",
    "JOIN \n",
    "    Genre g ON t.GenreId = g.GenreId\n",
    "GROUP BY \n",
    "    g.Name\n",
    "ORDER BY \n",
    "    TotalRevenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "genre_revenue = pd.read_sql_query(query, conn)\n",
    "genre_revenue.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25616cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=genre_revenue, x='Genre', y='TotalRevenue', palette='magma')\n",
    "plt.title('Total Revenue by Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    ar.Name AS Artist,\n",
    "    ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n",
    "    COUNT(il.InvoiceLineId) AS TotalSales\n",
    "FROM \n",
    "    InvoiceLine il\n",
    "JOIN \n",
    "    Track t ON il.TrackId = t.TrackId\n",
    "JOIN \n",
    "    Album al ON t.AlbumId = al.AlbumId\n",
    "JOIN \n",
    "    Artist ar ON al.ArtistId = ar.ArtistId\n",
    "GROUP BY \n",
    "    ar.Name\n",
    "ORDER BY \n",
    "    TotalRevenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "top_artists = pd.read_sql_query(query, conn)\n",
    "top_artists.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fcd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_artists, x='Artist', y='TotalRevenue', palette='cubehelix')\n",
    "plt.title('Top 10 Revenue-Generating Artists')\n",
    "plt.xlabel('Artist')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288017b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load Customer and Invoice tables\n",
    "customers = pd.read_sql_query(\"SELECT * FROM Customer\", conn)\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "\n",
    "# Set a fixed reference date (as if today) for Recency calculation\n",
    "reference_date = invoices['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Group by customer to calculate RFM\n",
    "rfm = invoices.groupby('CustomerId').agg({\n",
    "    'InvoiceDate': lambda x: (reference_date - x.max()).days,   # Recency\n",
    "    'InvoiceId': 'count',                                        # Frequency\n",
    "    'Total': 'sum'                                               # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "rfm.columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Merge with customer data for context\n",
    "rfm = rfm.merge(customers[['CustomerId', 'FirstName', 'LastName', 'Country']], on='CustomerId')\n",
    "\n",
    "rfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Select RFM values\n",
    "rfm_values = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "\n",
    "# Step 2: Scale the values\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_values)\n",
    "\n",
    "# Step 3: Apply K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Step 4: Review clusters\n",
    "cluster_summary = rfm.groupby('Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': 'mean',\n",
    "    'CustomerId': 'count'\n",
    "}).rename(columns={'CustomerId': 'Count'}).reset_index()\n",
    "\n",
    "print(cluster_summary)\n",
    "\n",
    "# Optional: Visualize clusters\n",
    "sns.pairplot(rfm, hue='Cluster', vars=['Recency', 'Frequency', 'Monetary'], palette='Set2')\n",
    "plt.suptitle(\"Customer Clusters Based on RFM\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if rfm_df exists in the current environment\n",
    "try:\n",
    "    print(\"✅ 'rfm_df' exists.\")\n",
    "    \n",
    "    # Show the first few rows to confirm structure\n",
    "    display(rfm_df.head())\n",
    "\n",
    "    # Check for essential columns\n",
    "    required_columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary', 'Cluster']\n",
    "    missing_columns = [col for col in required_columns if col not in rfm_df.columns]\n",
    "\n",
    "    if not missing_columns:\n",
    "        print(\"✅ All required columns are present:\", required_columns)\n",
    "    else:\n",
    "        print(\"⚠️ Missing columns:\", missing_columns)\n",
    "\n",
    "except NameError:\n",
    "    print(\"❌ 'rfm_df' does not exist. You may need to define it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clusters with customer details\n",
    "clustered_customers = pd.merge(rfm_df, customers, on='CustomerId')\n",
    "\n",
    "# Save each cluster's customers to separate CSVs\n",
    "for cluster in clustered_customers['Cluster'].unique():\n",
    "    cluster_df = clustered_customers[clustered_customers['Cluster'] == cluster]\n",
    "    filename = f\"cluster_{cluster}_customers.csv\"\n",
    "    cluster_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved customers in Cluster {cluster} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b665a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clusters with customer details\n",
    "clustered_customers = pd.merge(rfm, rfm_cluster[['CustomerId', 'Cluster']], on='CustomerId')\n",
    "\n",
    "# Save each cluster's customers to separate CSVs\n",
    "for cluster in clustered_customers['Cluster'].unique():\n",
    "    cluster_data = clustered_customers[clustered_customers['Cluster'] == cluster]\n",
    "    filename = f\"cluster_{cluster}_customers.csv\"\n",
    "    cluster_data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(cluster_data)} customers in Cluster {cluster} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each cluster's customers to separate CSVs\n",
    "for cluster in rfm['Cluster'].unique():\n",
    "    cluster_data = rfm[rfm['Cluster'] == cluster]\n",
    "    filename = f\"cluster_{cluster}_customers.csv\"\n",
    "    cluster_data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(cluster_data)} customers in Cluster {cluster} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query\n",
    "query = '''\n",
    "SELECT c.Country, g.Name AS Genre, COUNT(il.TrackId) AS TrackPurchases \n",
    "FROM Customer c \n",
    "JOIN Invoice i ON c.CustomerId = i.CustomerId \n",
    "JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId \n",
    "JOIN Track t ON il.TrackId = t.TrackId \n",
    "JOIN Genre g ON t.GenreId = g.GenreId \n",
    "GROUP BY c.Country, g.Name \n",
    "ORDER BY c.Country, TrackPurchases DESC\n",
    "'''\n",
    "\n",
    "genre_by_country = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9724b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a pivot table for better visualization\n",
    "pivot_data = genre_by_country.pivot_table(\n",
    "    index='Country', \n",
    "    columns='Genre', \n",
    "    values='TrackPurchases',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# 2. Find top genres for each country\n",
    "top_genres_by_country = genre_by_country.sort_values(\n",
    "    ['Country', 'TrackPurchases'], \n",
    "    ascending=[True, False]\n",
    ").groupby('Country').head(3)\n",
    "\n",
    "# Display top 3 genres for each country\n",
    "print(\"Top 3 genres by country:\")\n",
    "for country in top_genres_by_country['Country'].unique():\n",
    "    country_data = top_genres_by_country[top_genres_by_country['Country'] == country]\n",
    "    print(f\"\\n{country}:\")\n",
    "    for _, row in country_data.iterrows():\n",
    "        print(f\"  {row['Genre']}: {row['TrackPurchases']} purchases\")\n",
    "\n",
    "# 3. Create a heatmap of genre popularity by country\n",
    "plt.figure(figsize=(16, 10))\n",
    "heatmap_data = pivot_data.copy()\n",
    "\n",
    "# Normalize by country to see relative preferences\n",
    "for country in heatmap_data.index:\n",
    "    country_total = heatmap_data.loc[country].sum()\n",
    "    if country_total > 0:  # Avoid division by zero\n",
    "        heatmap_data.loc[country] = heatmap_data.loc[country] / country_total * 100\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(heatmap_data, cmap=\"YlGnBu\", annot=False)\n",
    "plt.title('Genre Preference by Country (% of Country\\'s Total Purchases)', fontsize=16)\n",
    "plt.ylabel('Country', fontsize=12)\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('genre_country_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Create a bar chart for top 5 countries by purchase volume\n",
    "top_countries = genre_by_country.groupby('Country')['TrackPurchases'].sum().nlargest(5)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_countries.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 5 Countries by Total Track Purchases', fontsize=16)\n",
    "plt.ylabel('Number of Tracks Purchased', fontsize=12)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_countries_purchases.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Top genres overall\n",
    "top_genres_overall = genre_by_country.groupby('Genre')['TrackPurchases'].sum().nlargest(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_genres_overall.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Top 10 Genres by Total Purchases', fontsize=16)\n",
    "plt.ylabel('Number of Tracks Purchased', fontsize=12)\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_genres_overall.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InvoiceLine and Track tables\n",
    "invoice_lines = pd.read_sql_query(\"SELECT * FROM InvoiceLine\", conn)\n",
    "tracks = pd.read_sql_query(\"SELECT * FROM Track\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491521fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "# Load InvoiceLine and Track tables\n",
    "invoice_lines = pd.read_sql_query(\"SELECT * FROM InvoiceLine\", conn)\n",
    "tracks = pd.read_sql_query(\"SELECT * FROM Track\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4081573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to include track names\n",
    "invoice_track = invoice_lines.merge(tracks[['TrackId', 'Name']], on='TrackId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85189d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transaction matrix\n",
    "basket = invoice_track.pivot_table(index='InvoiceId', \n",
    "                                   columns='Name', \n",
    "                                   values='Quantity', \n",
    "                                   aggfunc='sum', \n",
    "                                   fill_value=0)\n",
    "\n",
    "# Convert quantities to 1s and 0s (binary presence)\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e20819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Find frequent itemsets with a minimum support threshold\n",
    "frequent_itemsets = apriori(basket, min_support=0.02, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Sort rules by lift for inspection\n",
    "rules.sort_values('lift', ascending=False, inplace=True)\n",
    "\n",
    "rules.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3884e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Find frequent itemsets with a minimum support threshold\n",
    "frequent_itemsets = apriori(basket, min_support=0.02, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Sort rules by lift for inspection\n",
    "rules.sort_values('lift', ascending=False, inplace=True)\n",
    "\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: View most frequently purchased tracks\n",
    "basket.sum().sort_values(ascending=False).head(10).plot(kind='bar', title='Top Purchased Tracks')\n",
    "\n",
    "# Step 2: Generate frequent itemsets with lower min_support\n",
    "frequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Step 3: Generate association rules from frequent itemsets\n",
    "if not frequent_itemsets.empty:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "    rules.sort_values('lift', ascending=False, inplace=True)\n",
    "    display(rules)\n",
    "else:\n",
    "    print(\"⚠️ No frequent itemsets found. Try lowering the min_support further.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Generate frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Step 2: Generate association rules\n",
    "if not frequent_itemsets.empty:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "    rules.sort_values('lift', ascending=False, inplace=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    rules.to_csv(\"association_rules.csv\", index=False)\n",
    "    print(\"✅ Association rules saved to 'association_rules.csv'\")\n",
    "    \n",
    "    # Visualize: Lift vs Confidence\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(rules['confidence'], rules['lift'], alpha=0.7, c='green')\n",
    "    plt.title('Lift vs Confidence of Association Rules')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No frequent itemsets found. Try lowering the min_support.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fade267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 10 rules in a readable format\n",
    "print(\"📊 Top 10 Association Rules (by Lift):\\n\")\n",
    "top_rules = rules.head(10)\n",
    "\n",
    "for idx, row in top_rules.iterrows():\n",
    "    antecedent = ', '.join(list(row['antecedents']))\n",
    "    consequent = ', '.join(list(row['consequents']))\n",
    "    print(f\"If a customer buys [{antecedent}], they are also likely to buy [{consequent}]\")\n",
    "    print(f\" - Support: {row['support']:.2f}, Confidence: {row['confidence']:.2f}, Lift: {row['lift']:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Connect to the Chinook database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load necessary tables\n",
    "tracks = pd.read_sql_query(\"SELECT TrackId, Name, Milliseconds FROM Track\", conn)\n",
    "invoice_lines = pd.read_sql_query(\"SELECT TrackId FROM InvoiceLine\", conn)\n",
    "\n",
    "# Aggregate sales frequency per track\n",
    "sales_per_track = invoice_lines.groupby('TrackId').size().reset_index(name='SalesCount')\n",
    "\n",
    "# Merge with track duration info\n",
    "track_sales = pd.merge(tracks, sales_per_track, on='TrackId')\n",
    "\n",
    "# Convert duration to minutes for readability\n",
    "track_sales['DurationMinutes'] = track_sales['Milliseconds'] / (1000 * 60)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation, p_value = pearsonr(track_sales['DurationMinutes'], track_sales['SalesCount'])\n",
    "\n",
    "# Print correlation result\n",
    "print(f\"Pearson Correlation: {correlation:.3f} (p-value = {p_value:.3f})\")\n",
    "\n",
    "# Scatter plot with regression line\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(data=track_sales, x='DurationMinutes', y='SalesCount', scatter_kws={'alpha':0.6})\n",
    "plt.title('Correlation Between Track Duration and Sales Count')\n",
    "plt.xlabel('Track Duration (minutes)')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of Recency\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(rfm['Recency'], bins=30, kde=True)\n",
    "plt.title('Distribution of Customer Recency')\n",
    "plt.xlabel('Days Since Last Purchase')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5850f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of Recency using rfm_df\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(rfm_df['Recency'], bins=30, kde=True)\n",
    "plt.title('Distribution of Customer Recency')\n",
    "plt.xlabel('Days Since Last Purchase')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of Recency\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(rfm['Recency'], bins=30, kde=True)\n",
    "plt.title('Distribution of Customer Recency')\n",
    "plt.xlabel('Days Since Last Purchase')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define churn based on top 25% most inactive customers\n",
    "threshold = rfm['Recency'].quantile(0.75)\n",
    "rfm['Churn'] = (rfm['Recency'] > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RFM DataFrame\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load Customer and Invoice tables\n",
    "customers = pd.read_sql_query(\"SELECT * FROM Customer\", conn)\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "\n",
    "# Set a fixed reference date for Recency calculation\n",
    "reference_date = invoices['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Group by customer to calculate RFM\n",
    "rfm = invoices.groupby('CustomerId').agg({\n",
    "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
    "    'InvoiceId': 'count',                                     # Frequency\n",
    "    'Total': 'sum'                                            # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "rfm.columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Merge with customer data for context\n",
    "rfm = rfm.merge(customers[['CustomerId', 'FirstName', 'LastName', 'Country']], on='CustomerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22691b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of Recency\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(rfm['Recency'], bins=30, kde=True)\n",
    "plt.title('Distribution of Customer Recency')\n",
    "plt.xlabel('Days Since Last Purchase')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec162ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = rfm['Recency'].quantile(0.75)\n",
    "rfm['Churn'] = (rfm['Recency'] > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Features and target\n",
    "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "y = rfm['Churn']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4be8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load Invoice table\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "\n",
    "# Set InvoiceDate as index\n",
    "invoices.set_index('InvoiceDate', inplace=True)\n",
    "\n",
    "# Resample to monthly sales\n",
    "monthly_sales = invoices['Total'].resample('M').sum()\n",
    "\n",
    "# Preview\n",
    "print(monthly_sales.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2de951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform Dickey-Fuller test\n",
    "result = adfuller(monthly_sales)\n",
    "\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA(1,1,1)\n",
    "model = ARIMA(monthly_sales, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 6 months\n",
    "forecast = model_fit.forecast(steps=6)\n",
    "print(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Forecast the next 12 months\n",
    "forecast_steps = 12\n",
    "forecast = model_fit.get_forecast(steps=forecast_steps)\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create future dates\n",
    "last_date = monthly_sales.index[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
    "\n",
    "# Plot actual vs forecast\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_sales.index, monthly_sales['Total'], label='Actual Sales')\n",
    "plt.plot(future_dates, forecast.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(future_dates,\n",
    "                 forecast_ci['lower Total'],\n",
    "                 forecast_ci['upper Total'],\n",
    "                 color='lightgreen', alpha=0.4)\n",
    "plt.title(\"Sales Forecast (ARIMA)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Total\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales = monthly_sales.to_frame(name='Total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410981b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6681f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales = monthly_sales.to_frame(name='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load Invoice table\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "\n",
    "# Set InvoiceDate as index\n",
    "invoices.set_index('InvoiceDate', inplace=True)\n",
    "\n",
    "# Resample to monthly sales\n",
    "monthly_sales = invoices['Total'].resample('M').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64953562",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales = monthly_sales.to_frame(name='Total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1708c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_sales.index, monthly_sales['Total'], label='Actual Sales')\n",
    "plt.plot(future_dates, forecast.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(future_dates,\n",
    "                 forecast_ci['lower Total'],\n",
    "                 forecast_ci['upper Total'],\n",
    "                 color='lightgreen', alpha=0.4)\n",
    "\n",
    "plt.title('Sales Forecast for Next 12 Months')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# 2. Load Data\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "invoices.set_index('InvoiceDate', inplace=True)\n",
    "\n",
    "# 3. Resample to monthly sales\n",
    "monthly_sales = invoices['Total'].resample('M').sum()\n",
    "monthly_sales = monthly_sales.to_frame(name='Total')\n",
    "\n",
    "# 4. Fit ARIMA model\n",
    "model = ARIMA(monthly_sales['Total'], order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# 5. Forecast next 12 months\n",
    "forecast = model_fit.get_forecast(steps=12)\n",
    "forecast_ci = forecast.conf_int()\n",
    "future_dates = pd.date_range(start=monthly_sales.index[-1] + pd.DateOffset(months=1), periods=12, freq='M')\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(monthly_sales.index, monthly_sales['Total'], label='Actual Sales')\n",
    "plt.plot(future_dates, forecast.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(future_dates,\n",
    "                 forecast_ci.iloc[:, 0],\n",
    "                 forecast_ci.iloc[:, 1],\n",
    "                 color='lightgreen', alpha=0.4)\n",
    "plt.title('Sales Forecast for Next 12 Months')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine forecast results into one DataFrame\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Forecast': forecast.predicted_mean,\n",
    "    'Lower Bound': forecast_ci.iloc[:, 0],\n",
    "    'Upper Bound': forecast_ci.iloc[:, 1]\n",
    "})\n",
    "\n",
    "# Export to CSV\n",
    "forecast_df.to_csv('sales_forecast_next_12_months.csv')\n",
    "print(\"✅ Forecast exported to 'sales_forecast_next_12_months.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8018961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load Invoice data\n",
    "invoices = pd.read_sql_query(\"SELECT * FROM Invoice\", conn)\n",
    "invoices['InvoiceDate'] = pd.to_datetime(invoices['InvoiceDate'])\n",
    "\n",
    "# Define reference date (e.g., max invoice date in dataset)\n",
    "reference_date = invoices['InvoiceDate'].max()\n",
    "\n",
    "# Create customer-level features\n",
    "customer_df = invoices.groupby('CustomerId').agg({\n",
    "    'InvoiceDate': [\n",
    "        lambda x: (reference_date - x.max()).days,   # Recency\n",
    "        lambda x: (x.max() - x.min()).days,          # Tenure\n",
    "        'count'                                      # Frequency\n",
    "    ],\n",
    "    'Total': 'sum'                                   # Total spend\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "customer_df.columns = ['Recency', 'Tenure', 'Frequency', 'TotalSpend']\n",
    "customer_df = customer_df.reset_index()\n",
    "\n",
    "# Define churn: Recency > 365 days → churned\n",
    "customer_df['Churn'] = (customer_df['Recency'] > 365).astype(int)\n",
    "\n",
    "customer_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Features and target\n",
    "X = customer_df[['Recency', 'Tenure', 'Frequency', 'TotalSpend']]\n",
    "y = customer_df['Churn']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to Chinook DB\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load necessary tables\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId, InvoiceDate, BillingCountry, Total FROM Invoice\", conn)\n",
    "invoice_line = pd.read_sql_query(\"SELECT InvoiceId FROM InvoiceLine\", conn)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "invoice['InvoiceDate'] = pd.to_datetime(invoice['InvoiceDate'])\n",
    "\n",
    "# Feature: Number of tracks per invoice\n",
    "track_count = invoice_line.groupby('InvoiceId').size().reset_index(name='track_count')\n",
    "\n",
    "# Merge with invoice data\n",
    "data = pd.merge(invoice, track_count, on='InvoiceId')\n",
    "\n",
    "# Feature Engineering: Extract year, month, day, and weekday\n",
    "data['Year'] = data['InvoiceDate'].dt.year\n",
    "data['Month'] = data['InvoiceDate'].dt.month\n",
    "data['Day'] = data['InvoiceDate'].dt.day\n",
    "data['Weekday'] = data['InvoiceDate'].dt.weekday\n",
    "\n",
    "# One-hot encode BillingCountry\n",
    "data = pd.get_dummies(data, columns=['BillingCountry'], drop_first=True)\n",
    "\n",
    "# Drop unneeded columns\n",
    "data.drop(columns=['InvoiceId', 'InvoiceDate', 'CustomerId'], inplace=True)\n",
    "\n",
    "# Preview processed data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0218e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = invoice_features.drop('Total', axis=1)\n",
    "y = invoice_features['Total']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372543e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = invoice_df.drop('Total', axis=1)\n",
    "y = invoice_df['Total']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = data.drop('Total', axis=1)\n",
    "y = data['Total']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b55eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'data' in locals()  # Returns True if `data` is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to Chinook DB\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load necessary tables\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId, InvoiceDate, BillingCountry, Total FROM Invoice\", conn)\n",
    "invoice_line = pd.read_sql_query(\"SELECT InvoiceId FROM InvoiceLine\", conn)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "invoice['InvoiceDate'] = pd.to_datetime(invoice['InvoiceDate'])\n",
    "\n",
    "# Feature: Number of tracks per invoice\n",
    "track_count = invoice_line.groupby('InvoiceId').size().reset_index(name='track_count')\n",
    "\n",
    "# Merge with invoice data\n",
    "data = pd.merge(invoice, track_count, on='InvoiceId')\n",
    "\n",
    "# Feature Engineering: Extract year, month, day, and weekday\n",
    "data['Year'] = data['InvoiceDate'].dt.year\n",
    "data['Month'] = data['InvoiceDate'].dt.month\n",
    "data['Day'] = data['InvoiceDate'].dt.day\n",
    "data['Weekday'] = data['InvoiceDate'].dt.weekday\n",
    "\n",
    "# One-hot encode BillingCountry\n",
    "data = pd.get_dummies(data, columns=['BillingCountry'], drop_first=True)\n",
    "\n",
    "# Drop unneeded columns\n",
    "data.drop(columns=['InvoiceId', 'InvoiceDate', 'CustomerId'], inplace=True)\n",
    "\n",
    "# Preview to confirm it's loaded\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b136b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = data.drop('Total', axis=1)\n",
    "y = data['Total']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature names and coefficients\n",
    "coefficients = model.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "coef_df = coef_df.sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Impact on Invoice Total')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472300ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load InvoiceLine, Invoice, and Track\n",
    "invoice_line = pd.read_sql_query(\"SELECT InvoiceId, TrackId FROM InvoiceLine\", conn)\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId FROM Invoice\", conn)\n",
    "tracks = pd.read_sql_query(\"SELECT TrackId, Name FROM Track\", conn)\n",
    "\n",
    "# Merge for complete info\n",
    "invoice_merged = pd.merge(invoice_line, invoice, on=\"InvoiceId\")\n",
    "user_track_df = invoice_merged.groupby(['CustomerId', 'TrackId']).size().unstack(fill_value=0)\n",
    "\n",
    "# Optional: Normalize by converting values to 1 (purchased or not)\n",
    "user_track_df = user_track_df.applymap(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6096a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose to get Track similarity\n",
    "track_similarity = cosine_similarity(user_track_df.T)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "track_similarity_df = pd.DataFrame(track_similarity, \n",
    "                                   index=user_track_df.columns, \n",
    "                                   columns=user_track_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47faf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_tracks(track_id, n=5):\n",
    "    # Get top N similar tracks\n",
    "    similar_scores = track_similarity_df[track_id].sort_values(ascending=False)[1:n+1]\n",
    "    similar_track_ids = similar_scores.index.tolist()\n",
    "    \n",
    "    # Map to track names\n",
    "    recommended_names = tracks[tracks['TrackId'].isin(similar_track_ids)]['Name'].values\n",
    "    return recommended_names\n",
    "\n",
    "# Example: recommend tracks similar to track ID 10\n",
    "print(\"Recommended Tracks:\", recommend_tracks(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "print(data.isnull().sum())\n",
    "print(data.describe())\n",
    "print(data.sample(5))  # Random 5 rows to spot weird entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load data\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId, InvoiceDate, BillingCountry, Total FROM Invoice\", conn)\n",
    "invoice_line = pd.read_sql_query(\"SELECT InvoiceId FROM InvoiceLine\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Process data\n",
    "invoice['InvoiceDate'] = pd.to_datetime(invoice['InvoiceDate'])\n",
    "track_count = invoice_line.groupby('InvoiceId').size().reset_index(name='track_count')\n",
    "data = pd.merge(invoice, track_count, on='InvoiceId')\n",
    "\n",
    "# Feature Engineering\n",
    "data['Year'] = data['InvoiceDate'].dt.year\n",
    "data['Month'] = data['InvoiceDate'].dt.month\n",
    "data['Day'] = data['InvoiceDate'].dt.day\n",
    "data['Weekday'] = data['InvoiceDate'].dt.weekday\n",
    "data = pd.get_dummies(data, columns=['BillingCountry'], drop_first=True)\n",
    "data.drop(columns=['InvoiceId', 'InvoiceDate', 'CustomerId'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "print(data.isnull().sum())\n",
    "print(data.describe())\n",
    "print(data.sample(5))  # Random 5 rows to spot weird entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04075458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to Chinook DB\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# Load necessary tables\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId, InvoiceDate, BillingCountry, Total FROM Invoice\", conn)\n",
    "invoice_line = pd.read_sql_query(\"SELECT InvoiceId FROM InvoiceLine\", conn)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "invoice['InvoiceDate'] = pd.to_datetime(invoice['InvoiceDate'])\n",
    "\n",
    "# Feature: Number of tracks per invoice\n",
    "track_count = invoice_line.groupby('InvoiceId').size().reset_index(name='track_count')\n",
    "\n",
    "# Merge with invoice data\n",
    "data = pd.merge(invoice, track_count, on='InvoiceId')\n",
    "\n",
    "# Feature Engineering: Extract year, month, day, and weekday\n",
    "data['Year'] = data['InvoiceDate'].dt.year\n",
    "data['Month'] = data['InvoiceDate'].dt.month\n",
    "data['Day'] = data['InvoiceDate'].dt.day\n",
    "data['Weekday'] = data['InvoiceDate'].dt.weekday\n",
    "\n",
    "# One-hot encode BillingCountry\n",
    "data = pd.get_dummies(data, columns=['BillingCountry'], drop_first=True)\n",
    "\n",
    "# Drop unneeded columns\n",
    "data.drop(columns=['InvoiceId', 'InvoiceDate', 'CustomerId'], inplace=True)\n",
    "\n",
    "# Confirm it's loaded\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_invoice_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Customer & Invoice data\n",
    "conn = sqlite3.connect(\"Chinook_Sqlite.sqlite\")\n",
    "invoice = pd.read_sql_query(\"SELECT InvoiceId, CustomerId, InvoiceDate, Total FROM Invoice\", conn)\n",
    "conn.close()\n",
    "\n",
    "invoice['InvoiceDate'] = pd.to_datetime(invoice['InvoiceDate'])\n",
    "\n",
    "# Today's date for recency\n",
    "snapshot_date = invoice['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# RFM calculation\n",
    "rfm = invoice.groupby('CustomerId').agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceId': 'count',\n",
    "    'Total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary']\n",
    "rfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.to_csv(\"rfm_customer_segments.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
